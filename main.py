import streamlit as st
import PyPDF2
import docx
import os
import json
import spacy
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split

# Carregar modelo de NLP
nlp = spacy.load("en_core_web_sm")

# Fun√ß√£o para extrair texto de PDFs
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() + "\n"
    return text

# Fun√ß√£o para extrair texto de arquivos DOCX
def extract_text_from_docx(docx_path):
    doc = docx.Document(docx_path)
    return "\n".join([para.text for para in doc.paragraphs])

# Treinamento de modelo de Machine Learning (modelo mais complexo)
def train_model():
    data = pd.DataFrame({
        'text': [
            "Experi√™ncia com Python, Machine Learning e SQL",
            "Conhecimento avan√ßado em Gest√£o de Projetos e SCRUM",
            "Trabalho com banco de dados Oracle e PostgreSQL",
            "Forma√ß√£o acad√™mica incompleta sem experi√™ncia na √°rea",
            "Experi√™ncia em suporte t√©cnico e atendimento ao cliente",
            "Desenvolvimento de aplica√ß√µes web com Django e Flask",
            "Nenhuma experi√™ncia profissional na √°rea de TI",
            "Engenheiro de software com experi√™ncia em arquitetura de sistemas"
        ],
        'label': ["BOM", "BOM", "BOM", "RUIM", "RUIM", "BOM", "RUIM", "BOM"]
    })
    vectorizer = TfidfVectorizer()
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    pipeline = make_pipeline(vectorizer, model)
    X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)
    pipeline.fit(X_train, y_train)
    return pipeline

model = train_model()

# Fun√ß√£o para classificar curr√≠culo como BOM ou RUIM
def classify_resume(text):
    return model.predict([text])[0]

# Interface Web com Streamlit
st.title("üìÇ Automa√ß√£o de Separa√ß√£o de Curr√≠culos")

# Upload de arquivos
uploaded_files = st.file_uploader("Envie os curr√≠culos (PDF ou DOCX)", type=["pdf", "docx"], accept_multiple_files=True)

# Definir palavras-chave personaliz√°veis
keywords = st.text_area("Palavras-chave para classificar curr√≠culos", "Python, Machine Learning, SQL, Gest√£o de Projetos, SCRUM, PostgreSQL").split(",")

if uploaded_files:
    resultados = []
    
    for file in uploaded_files:
        file_path = os.path.join("/tmp", file.name)
        with open(file_path, "wb") as f:
            f.write(file.getbuffer())
        
        # Extra√ß√£o de texto
        if file.type == "application/pdf":
            text = extract_text_from_pdf(file_path)
        else:
            text = extract_text_from_docx(file_path)
        
        # Classifica√ß√£o usando Machine Learning
        status = classify_resume(text)
        resultados.append({"Arquivo": file.name, "Classifica√ß√£o": status})
    
    # Exibir resultados
    st.write("### Resultados da Classifica√ß√£o")
    st.json(resultados)
    
    # Salvar em Google Sheets (Integra√ß√£o com Google Apps Script - Placeholder)
    st.write("Os resultados podem ser integrados ao Google Sheets.")
    
    st.success("Processo conclu√≠do! Resultados salvos.")
    
# Permitir at√© 5 usu√°rios simultaneamente
st.write("Acesso permitido para at√© 5 usu√°rios.")
